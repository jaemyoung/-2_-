{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnWsY4FzYC0Y"
      },
      "source": [
        "# Define the deep learning (neural network) model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "jKm3tqsMhDOA"
      },
      "outputs": [],
      "source": [
        "# fcnet.py: Define the deep learning (neural network) model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, dropout=None):\n",
        "        super(FCNet, self).__init__()\n",
        "        self.fc0 = nn.Linear(3, layers[0])\n",
        "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
        "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
        "        self.fc_final = nn.Linear(layers[-1], 1)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_dropout = True if dropout else False\n",
        "        if self.use_dropout:\n",
        "            print(f'Using dropout: {dropout}')\n",
        "            self.dropout0 = nn.Dropout(p=dropout)\n",
        "            self.dropout1 = nn.Dropout(p=dropout)\n",
        "            self.dropout2 = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout0 = nn.Identity()\n",
        "            self.dropout1 = nn.Identity()\n",
        "            self.dropout2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x, targets=None, epoch=None):\n",
        "        x = self.dropout0(F.relu(self.fc0(x)))\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        x = self.fc_final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def fcnet1(**kwargs):\n",
        "    return FCNet([256, 256, 256], **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWohJXZPYbY3"
      },
      "source": [
        "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7Kzc8PTjhDIH"
      },
      "outputs": [],
      "source": [
        "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def weighted_mse_loss(inputs, targets, weights=None):\n",
        "    loss = (inputs - targets) ** 2\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aunbes6NY_Pz"
      },
      "source": [
        "# Define some utility functions (not the focus of this course)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "f3XXH8Yik4M9"
      },
      "outputs": [],
      "source": [
        "# utils.py: Define some utility functions (not the focus of this course).\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal.windows import triang\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_batch_fmtstr(num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "def query_yes_no(question):\n",
        "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
        "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
        "    prompt = \" [Y/n] \"\n",
        "\n",
        "    while True:\n",
        "        print(question + prompt, end=':')\n",
        "        choice = input().lower()\n",
        "        if choice == '':\n",
        "            return valid['y']\n",
        "        elif choice in valid:\n",
        "            return valid[choice]\n",
        "        else:\n",
        "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
        "\n",
        "def prepare_folders(args):\n",
        "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
        "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
        "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
        "            shutil.rmtree(folders_util[-1])\n",
        "            print(folders_util[-1] + ' removed.')\n",
        "        else:\n",
        "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
        "    for folder in folders_util:\n",
        "        if not os.path.exists(folder):\n",
        "            print(f\"===> Creating folder: {folder}\")\n",
        "            os.mkdir(folder)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    lr = args.lr\n",
        "    for milestone in args.schedule:\n",
        "        lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def save_checkpoint(args, state, is_best, prefix=''):\n",
        "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        print(\"===> Saving current best checkpoint...\")\n",
        "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
        "\n",
        "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
        "    if torch.sum(v1) < 1e-10:\n",
        "        return matrix\n",
        "    if (v1 == 0.).any():\n",
        "        valid = (v1 != 0.)\n",
        "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
        "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
        "        return matrix\n",
        "\n",
        "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
        "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
        "\n",
        "def get_lds_kernel_window(kernel, ks, sigma):\n",
        "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
        "    half_ks = (ks - 1) // 2\n",
        "    if kernel == 'gaussian':\n",
        "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
        "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
        "    elif kernel == 'triang':\n",
        "        kernel_window = triang(ks)\n",
        "    else:\n",
        "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
        "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
        "\n",
        "    return kernel_window\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpbjtnotZUkk"
      },
      "source": [
        "# Define the data iterator (data loader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "gqK8H1UghC_v"
      },
      "outputs": [],
      "source": [
        "# datasets.py: Define the data iterator (data loader).\n",
        "from scipy.ndimage import convolve1d\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "# from utils import get_lds_kernel_window\n",
        "\n",
        "class AIbloc(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train', reweight='none',\n",
        "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
        "        self.split = split\n",
        "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
        "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.data.shape[0]\n",
        "        feature = self.data[index, :-1]\n",
        "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
        "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
        "        return feature, label, weight\n",
        "\n",
        "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
        "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
        "        assert reweight != 'none' if lds else True, \\\n",
        "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
        "\n",
        "        value_dict = {x: 0 for x in range(max_target)}\n",
        "        labels = self.data[:, -1].tolist()\n",
        "        # mbr\n",
        "        for label in labels:\n",
        "            value_dict[min(max_target - 1, int(label))] += 1\n",
        "        if reweight == 'sqrt_inv':\n",
        "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
        "        elif reweight == 'inverse':\n",
        "            value_dict = {k: np.clip(v, 5, 1000) for k, v in value_dict.items()}  # clip weights for inverse re-weight\n",
        "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
        "        if not len(num_per_label) or reweight == 'none':\n",
        "            return None\n",
        "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
        "\n",
        "        if lds:\n",
        "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
        "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
        "            smoothed_value = convolve1d(\n",
        "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
        "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
        "\n",
        "        weights = [np.float32(1 / x) for x in num_per_label]\n",
        "        scaling = len(weights) / np.sum(weights)\n",
        "        weights = [scaling * x for x in weights]\n",
        "        return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcuwqp5paPk3"
      },
      "source": [
        "# Set up some default configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "T7mtQacPhBoW"
      },
      "outputs": [],
      "source": [
        "# train.py, Part 1: Set up some default configurations.\n",
        "import time\n",
        "import argparse\n",
        "#import logging\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from scipy.stats import gmean\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
        "\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "# CPU only\n",
        "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
        "# imbalanced related\n",
        "# LDS\n",
        "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
        "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
        "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
        "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
        "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
        "\n",
        "# re-weighting: SQRT_INV / INV\n",
        "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
        "\n",
        "# training/optimization related\n",
        "parser.add_argument('--dataset', type=str, default='AIbloc', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
        "parser.add_argument('--data_dir', type=str, default='./aibloc.data', help='data directory')\n",
        "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
        "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
        "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
        "parser.add_argument('--gpu', type=int, default=None)\n",
        "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
        "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
        "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
        "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
        "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
        "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
        "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
        "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
        "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
        "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
        "# checkpoints\n",
        "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
        "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
        "\n",
        "parser.set_defaults(augment=True)\n",
        "args, unknown = parser.parse_known_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nWvzFoYJnwuF"
      },
      "outputs": [],
      "source": [
        "args.cpu_only = True # Use CPU to train/test models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtG61vP8ROkP"
      },
      "source": [
        "# Train 4 different models using the 4 options below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Wv3tQjBp0dG"
      },
      "outputs": [],
      "source": [
        "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
        "args.reweight = 'none'\n",
        "args.lds = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jYS6r4nyp36_"
      },
      "outputs": [],
      "source": [
        "# Option 2: To train the inverse weighting model:\n",
        "args.reweight = 'inverse'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fTYBWlx4p6R3"
      },
      "outputs": [],
      "source": [
        "# Option 3: To train the sqrt_inverse weighting model:\n",
        "args.reweight = 'sqrt_inv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "W2j6V3UEqSoP"
      },
      "outputs": [],
      "source": [
        "# Option 4: To train the Label Distribution Smoothing (LDS) model:\n",
        "args.reweight = 'sqrt_inv'\n",
        "args.lds = True\n",
        "args.lds_kernel = 'gaussian'\n",
        "args.lds_ks = 5 # 5\n",
        "args.lds_sigma = 2 # 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nBTkrzbRbjT"
      },
      "source": [
        "# If you do not want to re-train models, download some trained models and directly use the models to predict labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hbe77zCNJAr",
        "outputId": "682307d2-99ed-464f-a7b4-651f4048f338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained models already exist. No need to download them.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "if not os.path.exists('./trained_models'):\n",
        "    # Download trained models (a zip file)\n",
        "    gdown.download(\"https://drive.google.com/uc?export=download&id=1dY3m_LdZvCLNOU1HnjjpVO9Fa8-qc2vk\", output='./trained_models.zip', quiet=False)\n",
        "    # Unzip the file\n",
        "    print('Extracting downloaded models...')\n",
        "    with zipfile.ZipFile('./trained_models.zip') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    os.remove('./trained_models.zip')\n",
        "    print(\"Completed!\")\n",
        "else:\n",
        "    print('Trained models already exist. No need to download them.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WYMK-mUPGe6"
      },
      "source": [
        "# Once you downloaded the models, you could configure to evaluate the trained models without re-training. Again we have 4 options, corresponding to 4 different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lAPDCoCGOxKr"
      },
      "outputs": [],
      "source": [
        "# Option 1: Evaluate the basic model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.base.pth.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JxcaG4dFPO8v"
      },
      "outputs": [],
      "source": [
        "# Option 2: Evaluate the inverse weighting model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.inverse.pth.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lV45Fm6FPTsp"
      },
      "outputs": [],
      "source": [
        "# Option 3: Evaluate the sqrt_inverse weighting model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.sqrt_inv.pth.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9onIhlctPf4K"
      },
      "outputs": [],
      "source": [
        "# Option 4: Evaluate the Label Distribution Smoothing (LDS) model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.lds.pth.tar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-zCfrN5av1A"
      },
      "source": [
        "# Train/Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSrzhog1gxyY",
        "outputId": "1af50ab7-c97d-4be3-e3eb-d91761db9c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overwrite previous folder: checkpoint/bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64 ? [Y/n] :y\n",
            "checkpoint/bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64 removed.\n",
            "===> Creating folder: checkpoint/bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
            "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, cpu_only=True, data_dir='./aibloc.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=True, lds_kernel='gaussian', lds_ks=5, lds_sigma=2, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
            "Store name: bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
            "=====> Preparing data...\n",
            "Using re-weighting: [SQRT_INV]\n",
            "Using LDS: [GAUSSIAN] (5/2)\n",
            "Training data size: 8000\n",
            "Validation data size: 2000\n",
            "Test data size: 2234\n",
            "=====> Building model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/125]\tTime   1.34 (  1.34)\tData 1.3105 (1.3105)\tLoss (L1) 10.913 (10.913)\n",
            "Epoch: [0][ 10/125]\tTime   0.01 (  0.13)\tData 0.0003 (0.1231)\tLoss (L1) 7.119 (8.223)\n",
            "Epoch: [0][ 20/125]\tTime   0.01 (  0.07)\tData 0.0058 (0.0664)\tLoss (L1) 6.268 (7.417)\n",
            "Epoch: [0][ 30/125]\tTime   0.03 (  0.05)\tData 0.0252 (0.0469)\tLoss (L1) 5.307 (6.787)\n",
            "Epoch: [0][ 40/125]\tTime   0.01 (  0.04)\tData 0.0005 (0.0361)\tLoss (L1) 4.779 (6.517)\n",
            "Epoch: [0][ 50/125]\tTime   0.01 (  0.04)\tData 0.0063 (0.0295)\tLoss (L1) 5.871 (6.224)\n",
            "Epoch: [0][ 60/125]\tTime   0.01 (  0.03)\tData 0.0004 (0.0249)\tLoss (L1) 3.819 (6.179)\n",
            "Epoch: [0][ 70/125]\tTime   0.01 (  0.03)\tData 0.0028 (0.0217)\tLoss (L1) 6.886 (6.642)\n",
            "Epoch: [0][ 80/125]\tTime   0.01 (  0.03)\tData 0.0028 (0.0193)\tLoss (L1) 7.662 (6.572)\n",
            "Epoch: [0][ 90/125]\tTime   0.01 (  0.02)\tData 0.0018 (0.0175)\tLoss (L1) 3.488 (6.408)\n",
            "Epoch: [0][100/125]\tTime   0.01 (  0.02)\tData 0.0023 (0.0160)\tLoss (L1) 3.674 (6.225)\n",
            "Epoch: [0][110/125]\tTime   0.01 (  0.02)\tData 0.0020 (0.0148)\tLoss (L1) 3.250 (6.061)\n",
            "Epoch: [0][120/125]\tTime   0.01 (  0.02)\tData 0.0025 (0.0138)\tLoss (L1) 4.668 (5.994)\n",
            "Val: [ 0/32]\tTime  1.230 ( 1.230)\tLoss (MSE) 22.111 (22.111)\tLoss (L1) 3.653 (3.653)\n",
            "Val: [10/32]\tTime  0.003 ( 0.115)\tLoss (MSE) 12.356 (27.021)\tLoss (L1) 2.641 (3.922)\n",
            "Val: [20/32]\tTime  0.005 ( 0.062)\tLoss (MSE) 29.890 (23.725)\tLoss (L1) 4.815 (3.804)\n",
            "Val: [30/32]\tTime  0.004 ( 0.043)\tLoss (MSE) 4.104 (23.987)\tLoss (L1) 1.462 (3.825)\n",
            " * Overall: RMSE 23.946\tL1 3.825\tG-Mean 2.534\n",
            " * Many: RMSE 4.642\tL1 3.652\tG-Mean 2.373\n",
            " * Median: RMSE 9.170\tL1 7.845\tG-Mean 6.121\n",
            " * Low: RMSE 23.528\tL1 23.528\tG-Mean 23.528\n",
            "Best L1 Loss: 3.825\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #0: Train loss [5.9547]; Val loss: MSE [23.9462], L1 [3.8253], G-Mean [2.5343]\n",
            "Epoch: [1][  0/125]\tTime   1.32 (  1.32)\tData 1.3150 (1.3150)\tLoss (L1) 2.993 (2.993)\n",
            "Epoch: [1][ 10/125]\tTime   0.01 (  0.13)\tData 0.0056 (0.1227)\tLoss (L1) 3.565 (4.155)\n",
            "Epoch: [1][ 20/125]\tTime   0.01 (  0.07)\tData 0.0004 (0.0656)\tLoss (L1) 2.535 (3.792)\n",
            "Epoch: [1][ 30/125]\tTime   0.01 (  0.05)\tData 0.0003 (0.0466)\tLoss (L1) 3.163 (3.792)\n",
            "Epoch: [1][ 40/125]\tTime   0.01 (  0.04)\tData 0.0009 (0.0360)\tLoss (L1) 4.228 (4.090)\n",
            "Epoch: [1][ 50/125]\tTime   0.01 (  0.03)\tData 0.0003 (0.0293)\tLoss (L1) 5.847 (4.320)\n",
            "Epoch: [1][ 60/125]\tTime   0.01 (  0.03)\tData 0.0004 (0.0246)\tLoss (L1) 8.873 (5.180)\n",
            "Epoch: [1][ 70/125]\tTime   0.01 (  0.03)\tData 0.0024 (0.0214)\tLoss (L1) 4.780 (5.167)\n",
            "Epoch: [1][ 80/125]\tTime   0.01 (  0.02)\tData 0.0032 (0.0191)\tLoss (L1) 5.061 (5.163)\n",
            "Epoch: [1][ 90/125]\tTime   0.01 (  0.02)\tData 0.0036 (0.0172)\tLoss (L1) 3.690 (5.130)\n",
            "Epoch: [1][100/125]\tTime   0.01 (  0.02)\tData 0.0026 (0.0159)\tLoss (L1) 4.289 (5.075)\n",
            "Epoch: [1][110/125]\tTime   0.01 (  0.02)\tData 0.0025 (0.0146)\tLoss (L1) 3.969 (4.993)\n",
            "Epoch: [1][120/125]\tTime   0.01 (  0.02)\tData 0.0027 (0.0136)\tLoss (L1) 2.771 (4.888)\n",
            "Val: [ 0/32]\tTime  1.285 ( 1.285)\tLoss (MSE) 24.860 (24.860)\tLoss (L1) 4.065 (4.065)\n",
            "Val: [10/32]\tTime  0.003 ( 0.119)\tLoss (MSE) 11.428 (28.040)\tLoss (L1) 2.571 (4.087)\n",
            "Val: [20/32]\tTime  0.004 ( 0.064)\tLoss (MSE) 26.404 (23.671)\tLoss (L1) 4.442 (3.828)\n",
            "Val: [30/32]\tTime  0.001 ( 0.044)\tLoss (MSE) 6.532 (24.687)\tLoss (L1) 1.880 (3.918)\n",
            " * Overall: RMSE 24.618\tL1 3.915\tG-Mean 2.542\n",
            " * Many: RMSE 4.818\tL1 3.878\tG-Mean 2.575\n",
            " * Median: RMSE 9.918\tL1 8.663\tG-Mean 6.950\n",
            " * Low: RMSE 22.739\tL1 22.739\tG-Mean 22.739\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #1: Train loss [4.8831]; Val loss: MSE [24.6177], L1 [3.9154], G-Mean [2.5418]\n",
            "Epoch: [2][  0/125]\tTime   1.33 (  1.33)\tData 1.3178 (1.3178)\tLoss (L1) 3.757 (3.757)\n",
            "Epoch: [2][ 10/125]\tTime   0.01 (  0.13)\tData 0.0044 (0.1234)\tLoss (L1) 5.517 (4.057)\n",
            "Epoch: [2][ 20/125]\tTime   0.01 (  0.07)\tData 0.0030 (0.0661)\tLoss (L1) 3.820 (3.917)\n",
            "Epoch: [2][ 30/125]\tTime   0.01 (  0.05)\tData 0.0025 (0.0462)\tLoss (L1) 5.196 (4.100)\n",
            "Epoch: [2][ 40/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0365)\tLoss (L1) 5.132 (4.121)\n",
            "Epoch: [2][ 50/125]\tTime   0.01 (  0.04)\tData 0.0008 (0.0295)\tLoss (L1) 6.626 (4.035)\n",
            "Epoch: [2][ 60/125]\tTime   0.01 (  0.03)\tData 0.0006 (0.0247)\tLoss (L1) 4.229 (4.011)\n",
            "Epoch: [2][ 70/125]\tTime   0.01 (  0.03)\tData 0.0022 (0.0215)\tLoss (L1) 2.904 (3.933)\n",
            "Epoch: [2][ 80/125]\tTime   0.01 (  0.02)\tData 0.0021 (0.0192)\tLoss (L1) 5.569 (4.020)\n",
            "Epoch: [2][ 90/125]\tTime   0.01 (  0.02)\tData 0.0027 (0.0175)\tLoss (L1) 6.784 (3.991)\n",
            "Epoch: [2][100/125]\tTime   0.01 (  0.02)\tData 0.0043 (0.0160)\tLoss (L1) 3.401 (3.972)\n",
            "Epoch: [2][110/125]\tTime   0.01 (  0.02)\tData 0.0024 (0.0148)\tLoss (L1) 6.125 (4.401)\n",
            "Epoch: [2][120/125]\tTime   0.01 (  0.02)\tData 0.0027 (0.0138)\tLoss (L1) 3.460 (4.524)\n",
            "Val: [ 0/32]\tTime  1.251 ( 1.251)\tLoss (MSE) 58.896 (58.896)\tLoss (L1) 5.654 (5.654)\n",
            "Val: [10/32]\tTime  0.001 ( 0.117)\tLoss (MSE) 54.864 (55.723)\tLoss (L1) 5.742 (5.806)\n",
            "Val: [20/32]\tTime  0.001 ( 0.063)\tLoss (MSE) 79.785 (53.333)\tLoss (L1) 7.008 (5.565)\n",
            "Val: [30/32]\tTime  0.001 ( 0.044)\tLoss (MSE) 24.332 (49.906)\tLoss (L1) 3.147 (5.349)\n",
            " * Overall: RMSE 49.863\tL1 5.350\tG-Mean 3.365\n",
            " * Many: RMSE 7.179\tL1 5.439\tG-Mean 3.401\n",
            " * Median: RMSE 8.816\tL1 8.136\tG-Mean 7.377\n",
            " * Low: RMSE 10.921\tL1 10.921\tG-Mean 10.921\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #2: Train loss [4.5644]; Val loss: MSE [49.8628], L1 [5.3498], G-Mean [3.3654]\n",
            "Epoch: [3][  0/125]\tTime   1.38 (  1.38)\tData 1.3687 (1.3687)\tLoss (L1) 6.860 (6.860)\n",
            "Epoch: [3][ 10/125]\tTime   0.01 (  0.13)\tData 0.0095 (0.1277)\tLoss (L1) 3.169 (4.539)\n",
            "Epoch: [3][ 20/125]\tTime   0.01 (  0.07)\tData 0.0003 (0.0688)\tLoss (L1) 4.108 (4.234)\n",
            "Epoch: [3][ 30/125]\tTime   0.01 (  0.05)\tData 0.0025 (0.0476)\tLoss (L1) 2.159 (4.261)\n",
            "Epoch: [3][ 40/125]\tTime   0.01 (  0.04)\tData 0.0008 (0.0381)\tLoss (L1) 5.695 (4.032)\n",
            "Epoch: [3][ 50/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0307)\tLoss (L1) 4.686 (3.954)\n",
            "Epoch: [3][ 60/125]\tTime   0.01 (  0.03)\tData 0.0003 (0.0258)\tLoss (L1) 4.257 (3.946)\n",
            "Epoch: [3][ 70/125]\tTime   0.01 (  0.03)\tData 0.0024 (0.0225)\tLoss (L1) 3.891 (3.965)\n",
            "Epoch: [3][ 80/125]\tTime   0.01 (  0.03)\tData 0.0027 (0.0200)\tLoss (L1) 3.925 (4.015)\n",
            "Epoch: [3][ 90/125]\tTime   0.01 (  0.02)\tData 0.0043 (0.0181)\tLoss (L1) 5.842 (4.451)\n",
            "Epoch: [3][100/125]\tTime   0.01 (  0.02)\tData 0.0024 (0.0165)\tLoss (L1) 4.889 (4.442)\n",
            "Epoch: [3][110/125]\tTime   0.01 (  0.02)\tData 0.0025 (0.0153)\tLoss (L1) 4.819 (4.413)\n",
            "Epoch: [3][120/125]\tTime   0.01 (  0.02)\tData 0.0024 (0.0143)\tLoss (L1) 4.209 (4.363)\n",
            "Val: [ 0/32]\tTime  1.265 ( 1.265)\tLoss (MSE) 24.324 (24.324)\tLoss (L1) 4.107 (4.107)\n",
            "Val: [10/32]\tTime  0.001 ( 0.118)\tLoss (MSE) 12.299 (37.806)\tLoss (L1) 2.797 (4.633)\n",
            "Val: [20/32]\tTime  0.006 ( 0.063)\tLoss (MSE) 24.624 (29.387)\tLoss (L1) 4.358 (4.240)\n",
            "Val: [30/32]\tTime  0.004 ( 0.044)\tLoss (MSE) 6.162 (27.068)\tLoss (L1) 1.886 (4.096)\n",
            " * Overall: RMSE 26.957\tL1 4.089\tG-Mean 2.763\n",
            " * Many: RMSE 4.842\tL1 3.970\tG-Mean 2.848\n",
            " * Median: RMSE 13.326\tL1 12.191\tG-Mean 10.826\n",
            " * Low: RMSE 23.455\tL1 23.455\tG-Mean 23.455\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #3: Train loss [4.3940]; Val loss: MSE [26.9567], L1 [4.0892], G-Mean [2.7628]\n",
            "Epoch: [4][  0/125]\tTime   1.38 (  1.38)\tData 1.3679 (1.3679)\tLoss (L1) 4.190 (4.190)\n",
            "Epoch: [4][ 10/125]\tTime   0.01 (  0.13)\tData 0.0018 (0.1274)\tLoss (L1) 2.027 (3.922)\n",
            "Epoch: [4][ 20/125]\tTime   0.01 (  0.07)\tData 0.0026 (0.0682)\tLoss (L1) 2.649 (3.640)\n",
            "Epoch: [4][ 30/125]\tTime   0.01 (  0.05)\tData 0.0004 (0.0490)\tLoss (L1) 4.802 (3.708)\n",
            "Epoch: [4][ 40/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0374)\tLoss (L1) 3.545 (3.667)\n",
            "Epoch: [4][ 50/125]\tTime   0.01 (  0.04)\tData 0.0003 (0.0301)\tLoss (L1) 2.599 (3.587)\n",
            "Epoch: [4][ 60/125]\tTime   0.01 (  0.03)\tData 0.0008 (0.0253)\tLoss (L1) 2.220 (3.599)\n",
            "Epoch: [4][ 70/125]\tTime   0.01 (  0.03)\tData 0.0027 (0.0221)\tLoss (L1) 36.764 (4.098)\n",
            "Epoch: [4][ 80/125]\tTime   0.01 (  0.03)\tData 0.0022 (0.0198)\tLoss (L1) 4.210 (4.452)\n",
            "Epoch: [4][ 90/125]\tTime   0.01 (  0.02)\tData 0.0026 (0.0178)\tLoss (L1) 3.571 (4.626)\n",
            "Epoch: [4][100/125]\tTime   0.01 (  0.02)\tData 0.0061 (0.0163)\tLoss (L1) 4.233 (4.580)\n",
            "Epoch: [4][110/125]\tTime   0.01 (  0.02)\tData 0.0029 (0.0152)\tLoss (L1) 4.765 (4.523)\n",
            "Epoch: [4][120/125]\tTime   0.01 (  0.02)\tData 0.0020 (0.0141)\tLoss (L1) 2.357 (4.470)\n",
            "Val: [ 0/32]\tTime  1.268 ( 1.268)\tLoss (MSE) 22.100 (22.100)\tLoss (L1) 3.908 (3.908)\n",
            "Val: [10/32]\tTime  0.003 ( 0.119)\tLoss (MSE) 13.642 (36.786)\tLoss (L1) 3.193 (4.494)\n",
            "Val: [20/32]\tTime  0.003 ( 0.064)\tLoss (MSE) 23.073 (28.461)\tLoss (L1) 4.369 (4.176)\n",
            "Val: [30/32]\tTime  0.003 ( 0.044)\tLoss (MSE) 3.296 (25.427)\tLoss (L1) 1.238 (3.964)\n",
            " * Overall: RMSE 25.312\tL1 3.956\tG-Mean 2.641\n",
            " * Many: RMSE 4.567\tL1 3.762\tG-Mean 2.702\n",
            " * Median: RMSE 13.423\tL1 12.318\tG-Mean 10.934\n",
            " * Low: RMSE 26.765\tL1 26.765\tG-Mean 26.765\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #4: Train loss [4.4543]; Val loss: MSE [25.3124], L1 [3.9558], G-Mean [2.6409]\n",
            "Epoch: [5][  0/125]\tTime   1.33 (  1.33)\tData 1.2644 (1.2644)\tLoss (L1) 6.029 (6.029)\n",
            "Epoch: [5][ 10/125]\tTime   0.01 (  0.13)\tData 0.0033 (0.1181)\tLoss (L1) 3.508 (4.285)\n",
            "Epoch: [5][ 20/125]\tTime   0.01 (  0.07)\tData 0.0003 (0.0634)\tLoss (L1) 3.464 (3.834)\n",
            "Epoch: [5][ 30/125]\tTime   0.01 (  0.05)\tData 0.0032 (0.0461)\tLoss (L1) 1.739 (3.667)\n",
            "Epoch: [5][ 40/125]\tTime   0.01 (  0.04)\tData 0.0003 (0.0351)\tLoss (L1) 6.458 (3.703)\n",
            "Epoch: [5][ 50/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0284)\tLoss (L1) 3.647 (3.744)\n",
            "Epoch: [5][ 60/125]\tTime   0.01 (  0.03)\tData 0.0003 (0.0238)\tLoss (L1) 5.606 (3.765)\n",
            "Epoch: [5][ 70/125]\tTime   0.01 (  0.03)\tData 0.0023 (0.0207)\tLoss (L1) 6.398 (3.789)\n",
            "Epoch: [5][ 80/125]\tTime   0.01 (  0.02)\tData 0.0020 (0.0185)\tLoss (L1) 33.853 (4.181)\n",
            "Epoch: [5][ 90/125]\tTime   0.01 (  0.02)\tData 0.0028 (0.0168)\tLoss (L1) 5.680 (4.308)\n",
            "Epoch: [5][100/125]\tTime   0.01 (  0.02)\tData 0.0031 (0.0154)\tLoss (L1) 2.814 (4.395)\n",
            "Epoch: [5][110/125]\tTime   0.01 (  0.02)\tData 0.0021 (0.0142)\tLoss (L1) 1.979 (4.287)\n",
            "Epoch: [5][120/125]\tTime   0.01 (  0.02)\tData 0.0021 (0.0133)\tLoss (L1) 3.432 (4.234)\n",
            "Val: [ 0/32]\tTime  1.247 ( 1.247)\tLoss (MSE) 23.158 (23.158)\tLoss (L1) 4.078 (4.078)\n",
            "Val: [10/32]\tTime  0.001 ( 0.117)\tLoss (MSE) 11.583 (40.532)\tLoss (L1) 2.651 (4.930)\n",
            "Val: [20/32]\tTime  0.007 ( 0.063)\tLoss (MSE) 18.935 (31.642)\tLoss (L1) 3.620 (4.458)\n",
            "Val: [30/32]\tTime  0.003 ( 0.044)\tLoss (MSE) 12.819 (29.159)\tLoss (L1) 3.211 (4.299)\n",
            " * Overall: RMSE 28.993\tL1 4.285\tG-Mean 3.018\n",
            " * Many: RMSE 5.138\tL1 4.220\tG-Mean 3.051\n",
            " * Median: RMSE 13.835\tL1 12.651\tG-Mean 11.199\n",
            " * Low: RMSE 22.671\tL1 22.671\tG-Mean 22.671\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #5: Train loss [4.2070]; Val loss: MSE [28.9929], L1 [4.2848], G-Mean [3.0184]\n",
            "Epoch: [6][  0/125]\tTime   1.38 (  1.38)\tData 1.3580 (1.3580)\tLoss (L1) 7.698 (7.698)\n",
            "Epoch: [6][ 10/125]\tTime   0.01 (  0.14)\tData 0.0004 (0.1268)\tLoss (L1) 3.481 (8.002)\n",
            "Epoch: [6][ 20/125]\tTime   0.01 (  0.08)\tData 0.0072 (0.0679)\tLoss (L1) 3.057 (6.692)\n",
            "Epoch: [6][ 30/125]\tTime   0.01 (  0.06)\tData 0.0022 (0.0474)\tLoss (L1) 3.181 (5.854)\n",
            "Epoch: [6][ 40/125]\tTime   0.01 (  0.05)\tData 0.0006 (0.0372)\tLoss (L1) 3.334 (5.356)\n",
            "Epoch: [6][ 50/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0302)\tLoss (L1) 2.893 (5.041)\n",
            "Epoch: [6][ 60/125]\tTime   0.01 (  0.03)\tData 0.0012 (0.0254)\tLoss (L1) 3.340 (4.798)\n",
            "Epoch: [6][ 70/125]\tTime   0.01 (  0.03)\tData 0.0023 (0.0221)\tLoss (L1) 2.236 (4.638)\n",
            "Epoch: [6][ 80/125]\tTime   0.01 (  0.03)\tData 0.0023 (0.0197)\tLoss (L1) 5.711 (4.526)\n",
            "Epoch: [6][ 90/125]\tTime   0.01 (  0.03)\tData 0.0024 (0.0178)\tLoss (L1) 4.389 (4.389)\n",
            "Epoch: [6][100/125]\tTime   0.01 (  0.02)\tData 0.0021 (0.0163)\tLoss (L1) 4.583 (4.336)\n",
            "Epoch: [6][110/125]\tTime   0.01 (  0.02)\tData 0.0028 (0.0151)\tLoss (L1) 3.545 (4.306)\n",
            "Epoch: [6][120/125]\tTime   0.01 (  0.02)\tData 0.0026 (0.0140)\tLoss (L1) 5.657 (4.210)\n",
            "Val: [ 0/32]\tTime  1.341 ( 1.341)\tLoss (MSE) 23.159 (23.159)\tLoss (L1) 4.108 (4.108)\n",
            "Val: [10/32]\tTime  0.001 ( 0.125)\tLoss (MSE) 9.206 (42.625)\tLoss (L1) 2.380 (5.249)\n",
            "Val: [20/32]\tTime  0.005 ( 0.067)\tLoss (MSE) 12.916 (34.391)\tLoss (L1) 2.745 (4.745)\n",
            "Val: [30/32]\tTime  0.005 ( 0.047)\tLoss (MSE) 23.630 (32.501)\tLoss (L1) 4.660 (4.630)\n",
            " * Overall: RMSE 32.293\tL1 4.609\tG-Mean 3.316\n",
            " * Many: RMSE 5.560\tL1 4.597\tG-Mean 3.305\n",
            " * Median: RMSE 13.677\tL1 12.592\tG-Mean 11.297\n",
            " * Low: RMSE 23.571\tL1 23.571\tG-Mean 23.571\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #6: Train loss [4.1814]; Val loss: MSE [32.2929], L1 [4.6085], G-Mean [3.3156]\n",
            "Epoch: [7][  0/125]\tTime   2.20 (  2.20)\tData 2.1711 (2.1711)\tLoss (L1) 2.802 (2.802)\n",
            "Epoch: [7][ 10/125]\tTime   0.02 (  0.22)\tData 0.0070 (0.2054)\tLoss (L1) 5.331 (3.689)\n",
            "Epoch: [7][ 20/125]\tTime   0.01 (  0.12)\tData 0.0042 (0.1124)\tLoss (L1) 6.303 (3.717)\n",
            "Epoch: [7][ 30/125]\tTime   0.10 (  0.09)\tData 0.0861 (0.0802)\tLoss (L1) 2.551 (3.661)\n",
            "Epoch: [7][ 40/125]\tTime   0.01 (  0.07)\tData 0.0042 (0.0617)\tLoss (L1) 1.937 (3.432)\n",
            "Epoch: [7][ 50/125]\tTime   0.01 (  0.06)\tData 0.0042 (0.0506)\tLoss (L1) 6.903 (4.210)\n",
            "Epoch: [7][ 60/125]\tTime   0.01 (  0.05)\tData 0.0002 (0.0427)\tLoss (L1) 4.654 (4.423)\n",
            "Epoch: [7][ 70/125]\tTime   0.01 (  0.05)\tData 0.0032 (0.0372)\tLoss (L1) 4.415 (4.370)\n",
            "Epoch: [7][ 80/125]\tTime   0.01 (  0.04)\tData 0.0001 (0.0330)\tLoss (L1) 2.318 (4.215)\n",
            "Epoch: [7][ 90/125]\tTime   0.01 (  0.04)\tData 0.0026 (0.0296)\tLoss (L1) 6.836 (4.198)\n",
            "Epoch: [7][100/125]\tTime   0.01 (  0.03)\tData 0.0035 (0.0270)\tLoss (L1) 2.396 (4.109)\n",
            "Epoch: [7][110/125]\tTime   0.01 (  0.03)\tData 0.0023 (0.0248)\tLoss (L1) 4.362 (4.025)\n",
            "Epoch: [7][120/125]\tTime   0.01 (  0.03)\tData 0.0025 (0.0229)\tLoss (L1) 3.044 (3.926)\n",
            "Val: [ 0/32]\tTime  1.339 ( 1.339)\tLoss (MSE) 18.758 (18.758)\tLoss (L1) 3.771 (3.771)\n",
            "Val: [10/32]\tTime  0.007 ( 0.125)\tLoss (MSE) 8.563 (43.090)\tLoss (L1) 2.324 (5.128)\n",
            "Val: [20/32]\tTime  0.003 ( 0.067)\tLoss (MSE) 6.004 (33.645)\tLoss (L1) 2.010 (4.611)\n",
            "Val: [30/32]\tTime  0.001 ( 0.046)\tLoss (MSE) 20.623 (30.320)\tLoss (L1) 4.441 (4.341)\n",
            " * Overall: RMSE 30.104\tL1 4.318\tG-Mean 2.872\n",
            " * Many: RMSE 5.242\tL1 4.238\tG-Mean 2.857\n",
            " * Median: RMSE 14.308\tL1 12.889\tG-Mean 11.037\n",
            " * Low: RMSE 27.234\tL1 27.234\tG-Mean 27.234\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #7: Train loss [3.9118]; Val loss: MSE [30.1042], L1 [4.3179], G-Mean [2.8718]\n",
            "Epoch: [8][  0/125]\tTime   1.42 (  1.42)\tData 1.4005 (1.4005)\tLoss (L1) 4.227 (4.227)\n",
            "Epoch: [8][ 10/125]\tTime   0.01 (  0.14)\tData 0.0051 (0.1312)\tLoss (L1) 2.952 (3.150)\n",
            "Epoch: [8][ 20/125]\tTime   0.02 (  0.08)\tData 0.0134 (0.0704)\tLoss (L1) 2.958 (3.163)\n",
            "Epoch: [8][ 30/125]\tTime   0.01 (  0.06)\tData 0.0040 (0.0506)\tLoss (L1) 2.548 (3.093)\n",
            "Epoch: [8][ 40/125]\tTime   0.01 (  0.05)\tData 0.0022 (0.0386)\tLoss (L1) 4.453 (4.406)\n",
            "Epoch: [8][ 50/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0311)\tLoss (L1) 2.612 (4.501)\n",
            "Epoch: [8][ 60/125]\tTime   0.01 (  0.03)\tData 0.0008 (0.0262)\tLoss (L1) 1.950 (4.438)\n",
            "Epoch: [8][ 70/125]\tTime   0.01 (  0.03)\tData 0.0023 (0.0228)\tLoss (L1) 3.488 (4.359)\n",
            "Epoch: [8][ 80/125]\tTime   0.01 (  0.03)\tData 0.0020 (0.0203)\tLoss (L1) 5.360 (4.270)\n",
            "Epoch: [8][ 90/125]\tTime   0.01 (  0.03)\tData 0.0024 (0.0184)\tLoss (L1) 3.396 (4.221)\n",
            "Epoch: [8][100/125]\tTime   0.01 (  0.02)\tData 0.0025 (0.0168)\tLoss (L1) 4.093 (4.169)\n",
            "Epoch: [8][110/125]\tTime   0.01 (  0.02)\tData 0.0026 (0.0156)\tLoss (L1) 3.138 (4.102)\n",
            "Epoch: [8][120/125]\tTime   0.01 (  0.02)\tData 0.0024 (0.0145)\tLoss (L1) 5.004 (3.994)\n",
            "Val: [ 0/32]\tTime  1.323 ( 1.323)\tLoss (MSE) 17.082 (17.082)\tLoss (L1) 3.554 (3.554)\n",
            "Val: [10/32]\tTime  0.003 ( 0.123)\tLoss (MSE) 12.547 (43.226)\tLoss (L1) 2.862 (5.233)\n",
            "Val: [20/32]\tTime  0.007 ( 0.066)\tLoss (MSE) 4.720 (34.571)\tLoss (L1) 1.757 (4.739)\n",
            "Val: [30/32]\tTime  0.003 ( 0.046)\tLoss (MSE) 20.352 (30.241)\tLoss (L1) 4.404 (4.329)\n",
            " * Overall: RMSE 30.024\tL1 4.305\tG-Mean 2.855\n",
            " * Many: RMSE 5.223\tL1 4.220\tG-Mean 2.896\n",
            " * Median: RMSE 13.895\tL1 12.366\tG-Mean 10.258\n",
            " * Low: RMSE 28.545\tL1 28.545\tG-Mean 28.545\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #8: Train loss [3.9584]; Val loss: MSE [30.0241], L1 [4.3049], G-Mean [2.8550]\n",
            "Epoch: [9][  0/125]\tTime   1.42 (  1.42)\tData 1.4062 (1.4062)\tLoss (L1) 3.658 (3.658)\n",
            "Epoch: [9][ 10/125]\tTime   0.01 (  0.14)\tData 0.0043 (0.1312)\tLoss (L1) 3.057 (3.636)\n",
            "Epoch: [9][ 20/125]\tTime   0.01 (  0.08)\tData 0.0049 (0.0700)\tLoss (L1) 2.275 (3.219)\n",
            "Epoch: [9][ 30/125]\tTime   0.05 (  0.06)\tData 0.0439 (0.0499)\tLoss (L1) 3.103 (3.303)\n",
            "Epoch: [9][ 40/125]\tTime   0.01 (  0.05)\tData 0.0004 (0.0383)\tLoss (L1) 4.462 (4.351)\n",
            "Epoch: [9][ 50/125]\tTime   0.01 (  0.04)\tData 0.0004 (0.0310)\tLoss (L1) 4.886 (4.376)\n",
            "Epoch: [9][ 60/125]\tTime   0.01 (  0.03)\tData 0.0003 (0.0262)\tLoss (L1) 5.812 (4.169)\n",
            "Epoch: [9][ 70/125]\tTime   0.01 (  0.03)\tData 0.0029 (0.0228)\tLoss (L1) 4.324 (4.090)\n",
            "Epoch: [9][ 80/125]\tTime   0.01 (  0.03)\tData 0.0021 (0.0203)\tLoss (L1) 3.282 (3.926)\n",
            "Epoch: [9][ 90/125]\tTime   0.01 (  0.03)\tData 0.0024 (0.0184)\tLoss (L1) 7.724 (3.839)\n",
            "Epoch: [9][100/125]\tTime   0.01 (  0.02)\tData 0.0023 (0.0168)\tLoss (L1) 2.992 (3.810)\n",
            "Epoch: [9][110/125]\tTime   0.01 (  0.02)\tData 0.0028 (0.0155)\tLoss (L1) 3.250 (3.714)\n",
            "Epoch: [9][120/125]\tTime   0.01 (  0.02)\tData 0.0038 (0.0144)\tLoss (L1) 2.153 (3.647)\n",
            "Val: [ 0/32]\tTime  1.341 ( 1.341)\tLoss (MSE) 17.151 (17.151)\tLoss (L1) 3.556 (3.556)\n",
            "Val: [10/32]\tTime  0.001 ( 0.125)\tLoss (MSE) 14.786 (48.387)\tLoss (L1) 3.118 (5.629)\n",
            "Val: [20/32]\tTime  0.001 ( 0.067)\tLoss (MSE) 5.565 (40.185)\tLoss (L1) 1.668 (5.171)\n",
            "Val: [30/32]\tTime  0.001 ( 0.046)\tLoss (MSE) 29.515 (35.304)\tLoss (L1) 5.256 (4.701)\n",
            " * Overall: RMSE 35.070\tL1 4.679\tG-Mean 3.060\n",
            " * Many: RMSE 5.736\tL1 4.635\tG-Mean 2.973\n",
            " * Median: RMSE 14.049\tL1 12.403\tG-Mean 10.007\n",
            " * Low: RMSE 27.535\tL1 27.535\tG-Mean 27.535\n",
            "Best L1 Loss: 3.825\n",
            "Epoch #9: Train loss [3.6305]; Val loss: MSE [35.0695], L1 [4.6792], G-Mean [3.0597]\n",
            "========================================================================================================================\n",
            "Test best model on testset...\n",
            "Loaded best model, epoch 1, best val loss 3.8253\n",
            "Test: [ 0/35]\tTime  1.385 ( 1.385)\tLoss (MSE) 16.365 (16.365)\tLoss (L1) 2.809 (2.809)\n",
            "Test: [10/35]\tTime  0.014 ( 0.130)\tLoss (MSE) 66.854 (24.549)\tLoss (L1) 7.178 (3.735)\n",
            "Test: [20/35]\tTime  0.008 ( 0.070)\tLoss (MSE) 27.388 (23.277)\tLoss (L1) 4.165 (3.700)\n",
            "Test: [30/35]\tTime  0.005 ( 0.049)\tLoss (MSE) 5.173 (20.625)\tLoss (L1) 1.850 (3.494)\n",
            " * Overall: RMSE 19.512\tL1 3.422\tG-Mean 2.244\n",
            " * Many: RMSE 4.114\tL1 3.212\tG-Mean 2.081\n",
            " * Median: RMSE 8.127\tL1 7.375\tG-Mean 6.199\n",
            " * Low: RMSE 18.526\tL1 18.526\tG-Mean 18.526\n",
            "Test loss: MSE [19.5115], L1 [3.4219], G-Mean [2.2440]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# train.py Part 2: Train/Evaluate the model.\n",
        "args.store_name = ''\n",
        "args.start_epoch, args.best_loss = 0, 1e5\n",
        "\n",
        "if len(args.store_name):\n",
        "    args.store_name = f'_{args.store_name}'\n",
        "if not args.lds and args.reweight != 'none':\n",
        "    args.store_name += f'_{args.reweight}'\n",
        "if args.lds:\n",
        "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
        "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
        "        args.store_name += f'_{args.lds_sigma}'\n",
        "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
        "\n",
        "prepare_folders(args)\n",
        "\n",
        "print(f\"Args: {args}\")\n",
        "print(f\"Store name: {args.store_name}\")\n",
        "\n",
        "def main():\n",
        "    if args.gpu is not None:\n",
        "        print(f\"Use GPU: {args.gpu} for training\")\n",
        "\n",
        "    # Data\n",
        "    print('=====> Preparing data...')\n",
        "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
        "\n",
        "    train_dataset = AIbloc(data_dir=args.data_dir+'.train', split='train',\n",
        "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
        "    val_dataset = AIbloc(data_dir=args.data_dir+'.val', split='val')\n",
        "    test_dataset = AIbloc(data_dir=args.data_dir+'.test', split='test')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                             num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    print(f\"Training data size: {len(train_dataset)}\")\n",
        "    print(f\"Validation data size: {len(val_dataset)}\")\n",
        "    print(f\"Test data size: {len(test_dataset)}\")\n",
        "\n",
        "    # Random Seed\n",
        "    np.random.seed(999)\n",
        "    # random.seed(999)\n",
        "    torch.manual_seed(999)\n",
        "\n",
        "    # Model\n",
        "    print('=====> Building model...')\n",
        "    model = fcnet1()\n",
        "    if not args.cpu_only:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # evaluate only\n",
        "    if args.evaluate:\n",
        "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
        "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
        "        return\n",
        "\n",
        "    # Loss and optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
        "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
        "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
        "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            args.best_loss = checkpoint['best_loss']\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
        "        else:\n",
        "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
        "\n",
        "    if not args.cpu_only:\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epoch):\n",
        "        adjust_learning_rate(optimizer, epoch, args)\n",
        "        train_loss = train(train_loader, model, optimizer, epoch)\n",
        "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
        "\n",
        "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
        "        is_best = loss_metric < args.best_loss\n",
        "        args.best_loss = min(loss_metric, args.best_loss)\n",
        "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
        "        save_checkpoint(args, {\n",
        "            'epoch': epoch + 1,\n",
        "            'model': args.model,\n",
        "            'best_loss': args.best_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best)\n",
        "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
        "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
        "\n",
        "    # test with best checkpoint\n",
        "    print(\"=\" * 120)\n",
        "    print(\"Test best model on testset...\")\n",
        "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
        "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
        "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch):\n",
        "    batch_time = AverageMeter('Time', ':6.2f')\n",
        "    data_time = AverageMeter('Data', ':6.4f')\n",
        "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch)\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "        if not args.cpu_only:\n",
        "            inputs, targets, weights = \\\n",
        "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
        "        outputs = model(inputs, targets, epoch)\n",
        "\n",
        "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
        "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
        "\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if idx % args.print_freq == 0:\n",
        "            progress.display(idx)\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
        "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [batch_time, losses_mse, losses_l1],\n",
        "        prefix=f'{prefix}: '\n",
        "    )\n",
        "\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "    criterion_gmean = nn.L1Loss(reduction='none')\n",
        "\n",
        "    model.eval()\n",
        "    losses_all = []\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
        "            if not args.cpu_only:\n",
        "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            preds.extend(outputs.data.cpu().numpy())\n",
        "            labels.extend(targets.data.cpu().numpy())\n",
        "\n",
        "            loss_mse = criterion_mse(outputs, targets)\n",
        "            loss_l1 = criterion_l1(outputs, targets)\n",
        "            loss_all = criterion_gmean(outputs, targets)\n",
        "            losses_all.extend(loss_all.cpu().numpy())\n",
        "\n",
        "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
        "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if idx % args.print_freq == 0:\n",
        "                progress.display(idx)\n",
        "\n",
        "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
        "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
        "        print(f\" * Overall: RMSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
        "        print(f\" * Many: RMSE {shot_dict['many']['rmse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
        "        print(f\" * Median: RMSE {shot_dict['median']['rmse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
        "        print(f\" * Low: RMSE {shot_dict['low']['rmse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
        "\n",
        "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
        "\n",
        "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
        "    train_labels = np.array(train_labels).astype(int)\n",
        "\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "    elif isinstance(preds, np.ndarray):\n",
        "        pass\n",
        "    else:\n",
        "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
        "\n",
        "    labels = np.array(labels).astype(int)\n",
        "\n",
        "    train_class_count, test_class_count = [], []\n",
        "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
        "    for l in np.unique(labels):\n",
        "        train_class_count.append(len(train_labels[train_labels == l]))\n",
        "        test_class_count.append(len(labels[labels == l]))\n",
        "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
        "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
        "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
        "\n",
        "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
        "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
        "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
        "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
        "\n",
        "    for i in range(len(train_class_count)):\n",
        "        if train_class_count[i] > many_shot_thr:\n",
        "            many_shot_mse.append(mse_per_class[i])\n",
        "            many_shot_l1.append(l1_per_class[i])\n",
        "            many_shot_gmean += list(l1_all_per_class[i])\n",
        "            many_shot_cnt.append(test_class_count[i])\n",
        "        elif train_class_count[i] < low_shot_thr:\n",
        "            low_shot_mse.append(mse_per_class[i])\n",
        "            low_shot_l1.append(l1_per_class[i])\n",
        "            low_shot_gmean += list(l1_all_per_class[i])\n",
        "            low_shot_cnt.append(test_class_count[i])\n",
        "        else:\n",
        "            median_shot_mse.append(mse_per_class[i])\n",
        "            median_shot_l1.append(l1_per_class[i])\n",
        "            median_shot_gmean += list(l1_all_per_class[i])\n",
        "            median_shot_cnt.append(test_class_count[i])\n",
        "\n",
        "    shot_dict = defaultdict(dict)\n",
        "    shot_dict['many']['rmse'] = np.sqrt(np.sum(many_shot_mse) / np.sum(many_shot_cnt))\n",
        "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
        "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
        "    shot_dict['median']['rmse'] = np.sqrt(np.sum(median_shot_mse) / np.sum(median_shot_cnt))\n",
        "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
        "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
        "    shot_dict['low']['rmse'] = np.sqrt(np.sum(low_shot_mse) / np.sum(low_shot_cnt))\n",
        "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
        "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
        "\n",
        "    return shot_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "deep_imbalance_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}